darknet的损失函数被用在backward->向后传播层里面

yolo层是做出预测的层，定义在yolo_layer.c里面

已知YOLO最重要的几个网络层是conv卷积层， revolu层和yolo层，激活函数是leaky?

激活的具体方法定义在了activations.h中，还有配套的gradient一系列函数
常用的激活函数有Rectified Linear Unit(ReLU) - 用于隐层神经元输出
                    ## x = max(0,x)
                Sigmoid（也是logistic） - 用于隐层神经元输出
                    ## x = 1./(1.+exp(-x))
                Softmax - 用于多分类神经网络输出 //yolo放弃了这个方法
                Linear - 用于回归神经网络输出（或二分类问题）
                    ## x = x
                leaky - YOLO经常见到
                       ## x = （x>0）?x: .1*x
        对应的gradient是他们的导函数，体现了反向传播所需要的梯度


batchnorm顾名思义，大概是给这一整个batch做一个正则化，防止单张图片的不确定性？？经常出现对bisa的修改，大概于此有关
